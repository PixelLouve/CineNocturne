{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c956d902",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\loulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\loulo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2025/08/27 16:44:26 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n",
      "c:\\Users\\loulo\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [16:44:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "2025/08/27 16:44:28 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "c:\\Users\\loulo\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1028: UserWarning: [16:44:29] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\c_api\\c_api.cc:1427: Saving model in the UBJSON format as default.  You can use file extension: `json`, `ubj` or `deprecated` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n",
      "2025/08/27 16:44:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline classification terminé ! Accuracy: 0.832 | AUC: 0.5694185487226879\n",
      "✅ Modèle XGB sauvegardé !\n",
      "✅ TfidfVectorizer sauvegardé !\n",
      "✅ SVD sauvegardé !\n",
      "✅ TF-IDF matrix complète sauvegardée !\n",
      "✅ Movie index complet sauvegardé !\n",
      "✅ MultiLabelBinarizer sauvegardé !\n",
      "✅ StandardScaler pour l'année sauvegardé !\n",
      "✅ NearestNeighbors complet sauvegardé !\n",
      "✅ DataFrame complet sauvegardé !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import xgboost as xgb\n",
    "from sqlalchemy import create_engine\n",
    "import re\n",
    "import os\n",
    "import joblib\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# =========================\n",
    "# 1. Charger les données\n",
    "# =========================\n",
    "DATABASE_URL = \"mysql+pymysql://louve:%40Marley080922@mysql-louve.alwaysdata.net/louve_movies\"\n",
    "engine = create_engine(DATABASE_URL)\n",
    "query = \"SELECT movie_id, title, synopsis, rating, genres, release_year FROM movies WHERE synopsis IS NOT NULL\"\n",
    "df = pd.read_sql(query, engine)\n",
    "\n",
    "# Nettoyage texte\n",
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "df[\"synopsis_clean\"] = df[\"synopsis\"].apply(preprocess_text)\n",
    "\n",
    "# =========================\n",
    "# 2. TF-IDF + SVD sur tout le dataset\n",
    "# =========================\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words=\"english\")\n",
    "tfidf_matrix_full = vectorizer.fit_transform(df[\"synopsis_clean\"].fillna(\"\"))\n",
    "\n",
    "svd_full = TruncatedSVD(n_components=100, random_state=42)\n",
    "tfidf_svd_full = svd_full.fit_transform(tfidf_matrix_full)\n",
    "\n",
    "# =========================\n",
    "# 3. Genres et années\n",
    "# =========================\n",
    "df[\"genres_list\"] = df[\"genres\"].fillna(\"\").apply(lambda x: x.split(\"|\"))\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "genres_encoded_full = mlb.fit_transform(df[\"genres_list\"])\n",
    "\n",
    "scaler_year = StandardScaler()\n",
    "year_scaled_full = scaler_year.fit_transform(df[[\"release_year\"]].fillna(df[\"release_year\"].mean()))\n",
    "\n",
    "# =========================\n",
    "# 4. Nearest Neighbors sur full dataset\n",
    "# =========================\n",
    "nn_full = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\")\n",
    "nn_full.fit(tfidf_matrix_full)\n",
    "distances_full, indices_full = nn_full.kneighbors(tfidf_matrix_full, n_neighbors=6)\n",
    "neighbor_scores_full = 1 - distances_full[:, 1:]\n",
    "\n",
    "sim_mean_full = neighbor_scores_full.mean(axis=1)\n",
    "sim_max_full = neighbor_scores_full.max(axis=1)\n",
    "sim_min_full = neighbor_scores_full.min(axis=1)\n",
    "sim_std_full = neighbor_scores_full.std(axis=1)\n",
    "\n",
    "# =========================\n",
    "# 5. Échantillon pour entraînement XGB\n",
    "# =========================\n",
    "df_sample = df.sample(10_000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "tfidf_matrix_sample = vectorizer.transform(df_sample[\"synopsis_clean\"].fillna(\"\"))\n",
    "tfidf_svd_sample = svd_full.transform(tfidf_matrix_sample)\n",
    "\n",
    "genres_encoded_sample = mlb.transform(df_sample[\"genres_list\"])\n",
    "year_scaled_sample = scaler_year.transform(df_sample[[\"release_year\"]].fillna(df[\"release_year\"].mean()))\n",
    "\n",
    "nn_sample = NearestNeighbors(metric=\"cosine\", algorithm=\"brute\")\n",
    "nn_sample.fit(tfidf_matrix_sample)\n",
    "distances_sample, indices_sample = nn_sample.kneighbors(tfidf_matrix_sample, n_neighbors=6)\n",
    "neighbor_scores_sample = 1 - distances_sample[:, 1:]\n",
    "\n",
    "sim_mean_sample = neighbor_scores_sample.mean(axis=1)\n",
    "sim_max_sample = neighbor_scores_sample.max(axis=1)\n",
    "sim_min_sample = neighbor_scores_sample.min(axis=1)\n",
    "sim_std_sample = neighbor_scores_sample.std(axis=1)\n",
    "\n",
    "# Classification : like / dislike\n",
    "threshold = 7.0  # note >= 7 -> \"like\"\n",
    "y_class_sample = (df_sample[\"rating\"] >= threshold).astype(int)\n",
    "\n",
    "# Features\n",
    "X_sample = np.column_stack([\n",
    "    tfidf_svd_sample,\n",
    "    genres_encoded_sample,\n",
    "    year_scaled_sample,\n",
    "    sim_mean_sample,\n",
    "    sim_max_sample,\n",
    "    sim_min_sample,\n",
    "    sim_std_sample\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sample, y_class_sample, test_size=0.2, random_state=42)\n",
    "\n",
    "# =========================\n",
    "# 6. MLflow & XGB\n",
    "# =========================\n",
    "mlflow.set_experiment(\"movies_reco_pipeline_classif\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"xgb_hybrid_classif\"):\n",
    "    params = {\n",
    "        \"n_estimators\": 300,\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "        \"use_label_encoder\": False,\n",
    "        \"eval_metric\": \"logloss\"\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Évaluation\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"auc\", auc)\n",
    "    mlflow.xgboost.log_model(model, \"xgb_classifier_model\")\n",
    "\n",
    "print(\"✅ Pipeline classification terminé ! Accuracy:\", acc, \"| AUC:\", auc)\n",
    "\n",
    "# =========================\n",
    "# 7. Sauvegarde des artefacts\n",
    "# =========================\n",
    "os.makedirs(\"model\", exist_ok=True)\n",
    "\n",
    "# Sauvegarde du modèle\n",
    "joblib.dump(model, \"model/xgb_classifier_model.joblib\")\n",
    "print(\"✅ Modèle XGB sauvegardé !\")\n",
    "\n",
    "# Sauvegarde du vectorizer\n",
    "joblib.dump(vectorizer, \"model/reco_vectorizer.joblib\")\n",
    "print(\"✅ TfidfVectorizer sauvegardé !\")\n",
    "\n",
    "# Sauvegarde du SVD\n",
    "joblib.dump(svd_full, \"model/svd_model.joblib\")\n",
    "print(\"✅ SVD sauvegardé !\")\n",
    "\n",
    "# Sauvegarde de la TF-IDF matrix complète\n",
    "joblib.dump(tfidf_matrix_full, \"model/tfidf_matrix_full.joblib\")\n",
    "print(\"✅ TF-IDF matrix complète sauvegardée !\")\n",
    "\n",
    "# Sauvegarde du movie index\n",
    "df[[\"title\"]].to_csv(\"model/movie_index.csv\", index=False)\n",
    "print(\"✅ Movie index complet sauvegardé !\")\n",
    "\n",
    "# Sauvegarde du MultiLabelBinarizer\n",
    "joblib.dump(mlb, \"model/mlb_model.joblib\")\n",
    "print(\"✅ MultiLabelBinarizer sauvegardé !\")\n",
    "\n",
    "# Sauvegarde du StandardScaler pour l'année\n",
    "joblib.dump(scaler_year, \"model/scaler_year.joblib\")\n",
    "print(\"✅ StandardScaler pour l'année sauvegardé !\")\n",
    "\n",
    "# Sauvegarde du NearestNeighbors complet\n",
    "joblib.dump(nn_full, \"model/nn_full.joblib\")\n",
    "print(\"✅ NearestNeighbors complet sauvegardé !\")\n",
    "\n",
    "# Sauvegarde du DataFrame complet\n",
    "df.to_csv(\"model/movies_full.csv\", index=False)\n",
    "print(\"✅ DataFrame complet sauvegardé !\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
